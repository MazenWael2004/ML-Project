{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525c2825ca42294b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:07:21.753040Z",
     "start_time": "2025-12-08T00:07:21.609909Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c89f0ca4d5f87a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:07:21.764817Z",
     "start_time": "2025-12-08T00:07:21.761056Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "DATASET_PATH = BASE_DIR / 'dataset'\n",
    "CATEGORIES = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "TARGET_COUNT = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f20741073e417d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:07:21.827572Z",
     "start_time": "2025-12-08T00:07:21.812018Z"
    }
   },
   "outputs": [],
   "source": [
    "def getImages(categoryPath):\n",
    "    return list(categoryPath.glob('*.jpg'))\n",
    "\n",
    "\n",
    "def printCategoryCounts():\n",
    "    print(\"\\nImage counts per category:\")\n",
    "    print(\"-\" * 30)\n",
    "    for categoryName in CATEGORIES:\n",
    "        categoryPath = DATASET_PATH / categoryName\n",
    "        imageCount = len(getImages(categoryPath))\n",
    "        print(f\"{categoryName:15s}: {imageCount} images\")\n",
    "\n",
    "\n",
    "def validateAndCleanDataset():\n",
    "    if not DATASET_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at: {DATASET_PATH}\")\n",
    "    print(\"Validating dataset for corrupted images...\")\n",
    "    categoryDirectories = [directory for directory in DATASET_PATH.iterdir() if directory.is_dir()]\n",
    "    print(f\"Found {len(categoryDirectories)} categories: {[category.name for category in categoryDirectories]}\\n\")\n",
    "    totalRemovedImages = 0\n",
    "    for categoryDirectory in sorted(categoryDirectories):\n",
    "        imageFiles = [file for file in categoryDirectory.iterdir() if file.is_file()]\n",
    "        removedCount = 0\n",
    "        for imagePath in imageFiles:\n",
    "            if cv2.imread(str(imagePath)) is None:\n",
    "                print(f\"Removing: {imagePath.name} as it is corrupted.\")\n",
    "                try:\n",
    "                    os.remove(imagePath)\n",
    "                    removedCount += 1\n",
    "                except Exception as error:\n",
    "                    print(f\"Warning: Could not remove: {error}\")\n",
    "        numberOfValidImages = len(imageFiles) - removedCount\n",
    "        print(f\"{categoryDirectory.name}: {numberOfValidImages}/{len(imageFiles)} valid images\")\n",
    "        totalRemovedImages += removedCount\n",
    "    print(f\"\\nValidation complete. Removed {totalRemovedImages} corrupted file(s).\")\n",
    "    printCategoryCounts()\n",
    "\n",
    "\n",
    "def augmentImage(image):\n",
    "    height, width = image.shape[:2]\n",
    "    center = (width / 2, height / 2)\n",
    "    augmentations = [\n",
    "        cv2.flip(image, 1),  # Horizontal flip\n",
    "        cv2.flip(image, 0),  # Vertical flip\n",
    "        cv2.warpAffine(image, cv2.getRotationMatrix2D(center, 90, 1.0), (width, height)), # 90 degrees\n",
    "        cv2.warpAffine(image, cv2.getRotationMatrix2D(center, 180, 1.0), (width, height)), # 180 degrees\n",
    "        cv2.warpAffine(image, cv2.getRotationMatrix2D(center, 270, 1.0), (width, height)), # 270 degrees\n",
    "        cv2.convertScaleAbs(image, alpha=1.3, beta=30),   # Brightness +30\n",
    "        cv2.convertScaleAbs(image, alpha=0.7, beta=-30),  # Brightness -30\n",
    "        cv2.GaussianBlur(image, (5, 5), 0),               # Gaussian blur 5x5\n",
    "    ]\n",
    "    # Zoom crop\n",
    "    scale = 1.2\n",
    "    newHeight, newWidth = int(height * scale), int(width * scale)\n",
    "    resized = cv2.resize(image, (newWidth, newHeight))\n",
    "    startHeight, startWidth = (newHeight - height) // 2, (newWidth - width) // 2\n",
    "    augmentations.append(resized[startHeight:startHeight + height, startWidth:startWidth + width])\n",
    "    return augmentations\n",
    "\n",
    "\n",
    "def augmentDataset():\n",
    "    print(f\"\\nAugmenting images to reach {TARGET_COUNT} per category...\")\n",
    "    for categoryName in CATEGORIES:\n",
    "        categoryPath = DATASET_PATH / categoryName\n",
    "        imagePaths = getImages(categoryPath)\n",
    "        currentImageCount = len(imagePaths)\n",
    "        imagesNeeded = TARGET_COUNT - currentImageCount\n",
    "        print(f\"\\n{categoryName}: {currentImageCount} images\", end=\"\")\n",
    "        if imagesNeeded <= 0:\n",
    "            print(\" - Already sufficient\")\n",
    "            continue\n",
    "        print(f\" (Need {imagesNeeded} more)\")\n",
    "        # Load original images using tqdm for progress\n",
    "        originalImages = [(cv2.imread(str(imageFile)), imageFile.stem) for imageFile in tqdm(imagePaths, desc=\"Loading\")]\n",
    "        originalImages = [(image, name) for image, name in originalImages]\n",
    "        # Generate augmented images\n",
    "        generatedCount = 0\n",
    "        while generatedCount < imagesNeeded:\n",
    "            image, imageName = random.choice(originalImages)\n",
    "            for augmentationIndex, augmentedImage in enumerate(augmentImage(image)):\n",
    "                if generatedCount >= imagesNeeded:\n",
    "                    break\n",
    "                savePath = categoryPath / f\"{imageName}_augmented_{generatedCount}_{augmentationIndex}.jpg\"\n",
    "                cv2.imwrite(str(savePath), augmentedImage)\n",
    "                generatedCount += 1\n",
    "        print(f\"Generated {generatedCount} augmented images\")\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Augmentation complete.\")\n",
    "    printCategoryCounts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b6232139b3f3d5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:07:56.199153Z",
     "start_time": "2025-12-08T00:07:21.873890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating dataset for corrupted images...\n",
      "Found 6 categories: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "\n",
      "cardboard: 500/500 valid images\n",
      "glass: 500/500 valid images\n",
      "metal: 500/500 valid images\n",
      "paper: 500/500 valid images\n",
      "plastic: 500/500 valid images\n",
      "trash: 500/500 valid images\n",
      "\n",
      "Validation complete. Removed 0 corrupted file(s).\n",
      "\n",
      "Image counts per category:\n",
      "------------------------------\n",
      "cardboard      : 500 images\n",
      "glass          : 500 images\n",
      "metal          : 500 images\n",
      "paper          : 500 images\n",
      "plastic        : 500 images\n",
      "trash          : 500 images\n"
     ]
    }
   ],
   "source": [
    "validateAndCleanDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e71037173f1d61b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:08:02.095509Z",
     "start_time": "2025-12-08T00:07:56.296679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmenting images to reach 500 per category...\n",
      "\n",
      "cardboard: 500 images - Already sufficient\n",
      "\n",
      "glass: 500 images - Already sufficient\n",
      "\n",
      "metal: 500 images - Already sufficient\n",
      "\n",
      "paper: 500 images - Already sufficient\n",
      "\n",
      "plastic: 500 images - Already sufficient\n",
      "\n",
      "trash: 500 images - Already sufficient\n",
      "\n",
      "==================================================\n",
      "Augmentation complete.\n",
      "\n",
      "Image counts per category:\n",
      "------------------------------\n",
      "cardboard      : 500 images\n",
      "glass          : 500 images\n",
      "metal          : 500 images\n",
      "paper          : 500 images\n",
      "plastic        : 500 images\n",
      "trash          : 500 images\n"
     ]
    }
   ],
   "source": [
    "augmentDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f040739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>cardboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>cardboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>cardboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>cardboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>cardboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...</td>\n",
       "      <td>trash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename   category\n",
       "0     c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...  cardboard\n",
       "1     c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...  cardboard\n",
       "2     c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...  cardboard\n",
       "3     c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...  cardboard\n",
       "4     c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...  cardboard\n",
       "...                                                 ...        ...\n",
       "2995  c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...      trash\n",
       "2996  c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...      trash\n",
       "2997  c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...      trash\n",
       "2998  c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...      trash\n",
       "2999  c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dat...      trash\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Extraction Step\n",
    "rows = []\n",
    "\n",
    "for category_dir in DATASET_PATH.iterdir(): # For each category directory\n",
    "    if category_dir.is_dir(): # Ensure it's a directory\n",
    "        category = category_dir.name # Get category name\n",
    "        for img in category_dir.iterdir(): # For each image in the category directory\n",
    "                rows.append({\n",
    "                    \"filename\": str(img),     \n",
    "                    \"category\": category\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "\n",
    "    hist = cv2.calcHist([img], [0,1,2], None, \n",
    "                         [8,8,8], [0,256,0,256,0,256])\n",
    "\n",
    "    return cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15f29fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dataset\\cardboard\n",
      "Processing: c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dataset\\glass\n",
      "Processing: c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dataset\\metal\n",
      "Processing: c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dataset\\paper\n",
      "Processing: c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dataset\\plastic\n",
      "Processing: c:\\Users\\AFAQE\\Documents\\GitHub\\ML-Project\\dataset\\trash\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATASET_PATH = BASE_DIR / \"dataset\"\n",
    "\n",
    "CATEGORIES = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "label_map = {name: idx for idx, name in enumerate(CATEGORIES)}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    folder = DATASET_PATH / category\n",
    "    print(\"Processing:\", folder)\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = folder / filename\n",
    "        features = extract_features(str(file_path))\n",
    "\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(label_map[category])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc32e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa65aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_val = scaler.transform(X_val)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f3c1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_val.npy\", y_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MSI-Env)",
   "language": "python",
   "name": "msi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
